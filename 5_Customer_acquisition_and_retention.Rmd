---
title: "Case Study 04: Customer Acquisition & Retention"
author: "Lee Waters"
date: "4/18/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)

# Define libraries
library(SMCRM)                                                                  # acquisitionRetrention data
library(reshape2)                                                               # melt correlation matrix
library(ggplot2)                                                                # visualize correlation matrix
library(tree)                                                                   # decision tree
library(caret)                                                                  # confusion matrix
library(randomForestSRC)                                                        # gridsearch
library(randomForest)                                                           # partialPlot
```

# I. Executive Summary
<!--
                                                                                Complete?
Key findings including:                                                         
- Important variables                                                           X
- Accuracy of final model                                                       X
-->

# II. The Problem
<!--
                                                                                Complete?
A. Introduction/background                                                      X              
B. Purpose of study/importance of study/statement of problem                    X              
C. Questions to be answered/conceptual statement of hypotheses                  X            
D. Outline of remainder of report (brief)                                       X           
-->

# III. Review of Related Literature
<!--
                                                                                Complete?
- Discuss in the context of the application (e.g. uses of logistic regression,  X
  SVM, for marketing/ targeted mailing)                                         
A. Acquaint reader with existing methodologies used in this area                X           
-->

# IV. Methodology
<!--
                                                                                Complete?

A. Identification, classification and operationalization of variables           X                                        
B. Statements of hypotheses being tested and/or models being developed          X           
C. Sampling techniques, if full data is not being used                          X     
D. Data collection process, including data sources, data size, etc.             X
    Primary/secondary?      
E. Modeling analysis/techniques used                                            X              
F. Methodological assumptions and limitations                                   X     
-->

```{r IV. Methodology (Read in data)}
# Read in data
data('acquisitionRetention')                                                    # original data
df = acquisitionRetention                                                       # data to mod
```

```{r, IV. Methodology (Modify variables}
# Identification, classification and operationalization of variables
str(df)                                                                         # init str

df$acquisition = as.factor(df$acquisition)                                      # num->factor
df$industry = as.factor(df$industry)                                            # num->factor 

str(df)                                                                         # final str
```

# V. Data
<!--
                                                                                Complete?
A. Data cleaning                                                                X        
B. Data pre-processing                                                          X  
C. Data limitations                                                             X
- Check correlations                                                            X
- Comment on distributions of data                                              X
- Imbalance in response?                                                        X
- Influential points                                                            X
- Handle using resampling or recoding?                                          X
- Train/test split                                                              X
-->

```{r V. Data (Missing values)}
# Check for missing values
sapply(df, function(x) sum(is.na(x)))                                           # initial missing 

# Data imputation
df = na.omit(df)                                                                # zero for first week 
sum(is.na(df))                                                                  # final missing
```

```{r V. Data (Correlations)}
# Check correlations                                                            # see source 1                                                                     
# Get lower triangle of the correlation matrix
  get_lower_tri<-function(cormat){
    cormat[upper.tri(cormat)] <- NA
    return(cormat)
  }
# Get upper triangle of the correlation matrix
  get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }

# Reorder the correlation matrix
reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
}  

# Reorder the correlation matrix
cormat = round(cor(df[c(5, 7, 14:15)]), 2)
cormat <- reorder_cormat(cormat)
upper_tri <- get_upper_tri(cormat)
# Melt the correlation matrix
melted_cormat <- melt(upper_tri, na.rm = TRUE)
# Create a ggheatmap
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation") +
  theme_minimal()+ # minimal theme
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()
# Print the heatmap
print(ggheatmap)
```

```{r V. Data (Imbalance)}
df_imbalance = as.factor(acquisitionRetention$acquisition)
plot(df_imbalance, xlab='Acquistion', ylab='Observations')
```

```{r V. Data (Influential Points)}
par(mfrow=c(2,2))

plot(df$acq_exp, ylab='Acquisition Expense')
plot(df$acq_exp_sq, ylab='Acquisition Expense Sq.')
plot(df$revenue, ylab='Profit')
plot(df$employees, ylab='Employees')
```

```{r V. Data (Train/Test Split)}
set.seed(42)
index = sample(nrow(df), .8*nrow(df))
train = df[index, ]
test = df[-index, ]
dim(train)
dim(test)
```

# VI. Findings (Results)
<!--
                                                                                Complete?
A. Results presented in tables or charts when appropriate                       X            
B. Results reported with respect to hypotheses/models                           X      
C. Facts kept separate from interpretation, inference and evaluation            X
  - Clearly talk about what data shows, such as dist. or causal relationships   X
    and secondly talk about inference, prediction accuracy, etc.)               
- Changing of cut-off threshold                                                 X
- Discussion of sensitivty and specificity                                      X
-->

```{r VI. Results (LR)}
(glm.fit = glm(acquisition ~ acq_exp+acq_exp_sq+industry+revenue+employees, data=train, family=binomial))

glm.probs = predict(glm.fit, newdata=test, type="response")

table(train$acquisition)
cutoff = 272/(128+272)

glm.preds = ifelse(glm.probs >= cutoff, 1, 0)
confusionMatrix(as.factor(glm.preds), test$acquisition, positive='1')
```

```{r VI. Results (DT)}
# Grow a tree
tree = tree(acquisition ~ acq_exp+acq_exp_sq+industry+revenue+employees, data=train)
summary(tree)
par(mfrow=c(1,3))
plot(tree)
#text(tree, pretty=0, digits=3)

# Prune tree
set.seed(42)
tree_cv = cv.tree(tree)
tree_cv
plot(tree_cv$size, tree_cv$dev, type='b')

tree_prune = prune.tree(tree, best=3)

plot(tree_prune)
text(tree_prune, pretty=0, digits=3)

# Predict val
dt.preds = predict(tree_prune, newdata=test, type="class")

# Compute accuracy
confusionMatrix(as.factor(dt.preds), test$acquisition, positive='1')
```

```{r VI. Results (RF Classification Train)}
set.seed(42)
(rf1 = randomForest(acquisition ~ acq_exp+acq_exp_sq+industry+revenue+employees, data=train))

# Predict val
rf1.preds = predict(rf1, newdata=test, type="class")

# Compute accuracy
confusionMatrix(as.factor(rf1.preds), test$acquisition, positive='1')
```

```{r, Results (RF Classification Full)}
set.seed(42)
rf2 = randomForest(acquisition ~ acq_exp+acq_exp_sq+industry+revenue+employees, data=df)
rf2

df$rf2 = rf2$predicted
df2 = df[df$acquisition==1 & df$rf2==1,]
dim(df2)

set.seed(42)
index = sample(nrow(df2), .8*nrow(df2))
train = df2[index, ]
test = df2[-index, ]
dim(train)
dim(test)
```

```{r VI. Results (RF Interaction)}
set.seed(42)
(rf3 = rfsrc(duration~profit+acq_exp+ret_exp+acq_exp_sq+ret_exp_sq+freq+freq_sq+crossbuy+sow+industry+revenue+employees, data=train, importance=T))

set.seed(42)
find.interaction(rf3, method="vimp", importance="permute")

set.seed(42)
(rf4 = rfsrc(duration~profit+acq_exp+ret_exp+acq_exp_sq+ret_exp_sq+freq+freq_sq+crossbuy+sow+industry+revenue+employees+ret_exp_sq:profit, data=train))
```

```{r VI. Results (RF Gridsearch), cache=TRUE}
# Establish a list of possible values for hyper-parameters
mtry.values = seq(4,6,1)
nodesize.values = seq(4,8,2)
ntree.values = seq(4e3,6e3,1e3)

# Create a data frame containing all combinations 
hyper_grid = expand.grid(mtry = mtry.values, nodesize = nodesize.values, ntree = ntree.values)

# Create an empty vector to store OOB error values
oob_err = c()

# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:nrow(hyper_grid)) {

  # Train a Random Forest model
  set.seed(42)
  model = rfsrc(duration ~ profit+acq_exp+ret_exp+acq_exp_sq+ret_exp_sq+freq+freq_sq+crossbuy+sow+industry+revenue+employees,
                       data=train,
                       mtry=hyper_grid$mtry[i],
                       nodesize=hyper_grid$nodesize[i],
                       ntree=hyper_grid$ntree[i])  
  
  # Store OOB error for the model                      
  oob_err[i] = model$err.rate[length(model$err.rate)]
}

# Identify optimal set of hyperparmeters based on OOB error
opt_i = which.min(oob_err)
print(hyper_grid[opt_i,])
```

```{r VI. Results (RF Evaluate)}
set.seed(42)
(rf5 = randomForest(duration ~ profit+acq_exp+ret_exp+acq_exp_sq+ret_exp_sq+freq+freq_sq+crossbuy+sow+industry+revenue+employees, data=train, mtry=6, nodesize=4, ntree=4000))

# Predict val
yhat_rf = predict(rf5, newdata=test)

# Compute val error
(mean((yhat_rf-test$duration)^2))

plot(x=yhat_rf, y=test$duration, xlab='yhat', ylab='y')
```

```{r VI. Results (RF PDP), cache=TRUE}
par(mfrow=c(1,3))
partialPlot(rf5, train, profit)
partialPlot(rf5, train, acq_exp)
partialPlot(rf5, train, ret_exp)
partialPlot(rf5, train, acq_exp_sq)
partialPlot(rf5, train, ret_exp_sq)
partialPlot(rf5, train, freq)
partialPlot(rf5, train, freq_sq)
partialPlot(rf5, train, crossbuy)
partialPlot(rf5, train, sow)
partialPlot(rf5, train, industry)
partialPlot(rf5, train, revenue)
partialPlot(rf5, train, employees)
```

# VII. Conclusions and Recommendations
<!--                                                                            # Complete?
A. Discuss alternative methodologies                                            X
-->

# VIII. Appendix
## References